### 评估方法

通常我们需要一个测试集来评估所训练出的模型，且测试样本尽可能地要不出现在训练样本中。当我们拥有数据集D时，就需要对D做一定的处理从而得到训练集S和测试集T。

以下是几种常见做法：

#### 一、留出法

##### a.基本做法

​	将数据集D划分为两个互斥的集合，训练集S和测试集T。用S中的数据来训练出模型，再用T中的数据来评估泛化误差。

##### b.实践注意点

​	1.训练集/测试集的划分要尽可能保持数据分布的一致性，避免因为数据划分过程引入额外的偏差而对最终结果产生影响。

(如：D有500个正例和500个反例，若S和T分别为800和200，则S应该有400个正例和反例，T有100个正例和反例)

​	2.对数据集D的不同划分方式将导致模型评估的结果也存在差别。因此单次留出法的结果不够稳定可靠，一般需要若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。

(如：D有500个正例，在排序后，我们既可以选择将前400个正例放入S中，也可以将后400个放入S中，而这将导致不同的结果)

​	3.留出法通常将D的2/3到4/5左右的样本用于训练，其余的用于测试。若S太多，T太小，则评估结果可能不准确；而若S太小，T太多，被评估的模型与用D训练的模型相比可能有较大差别。

#### 二、交叉验证法

##### a.基本做法

​	将数据集D划分为k个大小相似的互斥子集，<u>每个子集都尽可能保持数据分布的一致性。</u>然后每次用k-1个子集的并集作为训练集，余下那个子集作为测试集。这样便得到了k组训练/测试集，从而可以进行k次训练和测试，最终返回的是这k个测试结果的均值。

##### b.实践注意点

​	1.交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这点，通常把交叉验证法称为"K折交叉验证"。**k最常用的取值是10。**

​	2.与留出法类似，数据集D划分成k个子集也存在多种方式，为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分方式重复p次。

##### c.特例:留一法

​	留一法既数据集D有m个样本，同时k也为m。这样做的**好处**是：不受随机样本划分方式的影响，因为每个子集都只有一个样本；使用的训练集只比初始数据集少一个样本，因此被实际评估的模型会与期望用数据集D训练出的模型很相似。但也存在显著的**缺陷**：在数据集较大时，训练m个模型是难以接受的，需要极强的计算力。

#### 三、自助法

##### a.基本做法

​	对于包含m个样本的数据集D，每次随机从中选出一个样本拷贝入D'，再将该样本放回D中，使得该样本在下次采样中仍有可能被选到。这个过程重复m次后，将得到包含m个样本的数据集D'。随后将D'用于训练，D中未被D‘包含的样本用于测试。

##### b.实践注意点	

​	1.显然D中的部分样本将在D'中多次出现，而有的样本将不出现。一个样本在m次挑选后未被选中的概率为:(1-1/m)的m次方，极限为1/e。因此我们每次约有总量1/3的数据可用于测试。

​	2.自助法在数据集较小，难以有效划分训练/测试集时较有效。

​	3.自助法产生的数据集改变了初始数据集的分布，这将引入估计偏差。